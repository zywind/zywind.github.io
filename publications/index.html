<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Yunfeng Zhang | 张云峰 | publications</title>
    <meta name="author" content="Yunfeng  Zhang" />
    <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar." />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://zywind.github.io/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://zywind.github.io/">Yunfeng Zhang | 张云峰</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- CV -->
              <li class="nav-item">
                <a class="nav-link" href="/assets/pdf/resume.pdf" target="_blank">
                  CV
                </a>
              </li>

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CSCW</abbr></div>

        <!-- Entry bib key -->
        <div id="ghai_explainable_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Explainable Active Learning (XAL): Toward AI Explanations as Interfaces for Machine Teachers</div>
          <!-- Author -->
          <div class="author">Ghai, Bhavya, Liao, Q. Vera, 
                  <em>Zhang, Yunfeng</em>, Bellamy, Rachel, and Mueller, Klaus
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Proceedings of the ACM on Human-Computer Interaction</em> Jan 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The wide adoption of Machine Learning (ML) technologies has created a growing demand for people who can train ML models. Some advocated the term "machine teacher” to refer to the role of people who inject domain knowledge into ML models. This "teaching” perspective emphasizes supporting the productivity and mental wellbeing of machine teachers through efficient learning algorithms and thoughtful design of human-AI interfaces. One promising learning paradigm is Active Learning (AL), by which the model intelligently selects instances to query a machine teacher for labels, so that the labeling workload could be largely reduced. However, in current AL settings, the human-AI interface remains minimal and opaque. A dearth of empirical studies further hinders us from developing teacher-friendly interfaces for AL algorithms. In this work, we begin considering AI explanations as a core element of the human-AI interface for teaching machines. When a human student learns, it is a common pattern to present one’s own reasoning and solicit feedback from the teacher. When a ML model learns and still makes mistakes, the teacher ought to be able to understand the reasoning underlying its mistakes. When the model matures, the teacher should be able to recognize its progress in order to trust and feel confident about their teaching outcome. Toward this vision, we propose a novel paradigm of explainable active learning (XAL), by introducing techniques from the surging field of explainable AI (XAI) into an AL setting. We conducted an empirical study comparing the model learning outcomes, feedback content and experience with XAL, to that of traditional AL and coactive learning (providing the model’s prediction without explanation). Our study shows benefits of AI explanation as interfaces for machine teaching–supporting trust calibration and enabling rich forms of teaching feedback, and potential drawbacks–anchoring effect with the model judgment and additional cognitive workload. Our study also reveals important individual factors that mediate a machine teacher’s reception to AI explanations, including task knowledge, AI experience and Need for Cognition. By reflecting on the results, we suggest future directions and design implications for XAL, and more broadly, machine teaching through AI explanations.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IUI</abbr></div>

        <!-- Entry bib key -->
        <div id="narkar_model_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Model LineUpper: Supporting Interactive Model Comparison at Multiple Levels for AutoML</div>
          <!-- Author -->
          <div class="author">Narkar, Shweta, 
                  <em>Zhang, Yunfeng</em>, Liao, Q. Vera, Wang, Dakuo, and Weisz, Justin D.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 26th International Conference on Intelligent User Interfaces</em> Apr 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IUI</abbr></div>

        <!-- Entry bib key -->
        <div id="narkar_model_2022" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Model LineUpper: Supporting Interactive Model Comparison at Multiple Levels for AutoML</div>
          <!-- Author -->
          <div class="author">Narkar, Shweta, 
                  <em>Zhang, Yunfeng</em>, Liao, Q. Vera, Wang, Dakuo, and Weisz, Justin D.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 26th International Conference on Intelligent User Interfaces</em> Apr 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="natesan_ramamurthy_model_2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Model Agnostic Multilevel Explanations</div>
          <!-- Author -->
          <div class="author">Natesan Ramamurthy, Karthikeyan, Vinzamuri, Bhanukiran, 
                  <em>Zhang, Yunfeng</em>, and Dhurandhar, Amit
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> Apr 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AIES</abbr></div>

        <!-- Entry bib key -->
        <div id="sharma_data_2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Data Augmentation for Discrimination Prevention and Bias Disambiguation</div>
          <!-- Author -->
          <div class="author">Sharma, Shubham, 
                  <em>Zhang, Yunfeng</em>, Ríos Aliaga, Jesús M., Bouneffouf, Djallel, Muthusamy, Vinod, and Varshney, Kush R.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em> Feb 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an "ideal world” dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">FAccT</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_effect_2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, Liao, Q. Vera, and Bellamy, Rachel K. E.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em> Jan 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model’s to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people’s trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI’s errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AIES</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_joint_2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Joint Optimization of AI Fairness and Utility: A Human-Centered Approach</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, Bellamy, Rachel, and Varshney, Kush
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em> Feb 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers’ preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

        <!-- Entry bib key -->
        <div id="natesan_ramamurthy_model_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Model Agnostic Multilevel Explanations</div>
          <!-- Author -->
          <div class="author">Natesan Ramamurthy, Karthikeyan, Vinzamuri, Bhanukiran, 
                  <em>Zhang, Yunfeng</em>, and Dhurandhar, Amit
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> Feb 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AIES</abbr></div>

        <!-- Entry bib key -->
        <div id="sharma_data_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Data Augmentation for Discrimination Prevention and Bias Disambiguation</div>
          <!-- Author -->
          <div class="author">Sharma, Shubham, 
                  <em>Zhang, Yunfeng</em>, Ríos Aliaga, Jesús M., Bouneffouf, Djallel, Muthusamy, Vinod, and Varshney, Kush R.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em> Feb 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an "ideal world” dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">FAccT</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_effect_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, Liao, Q. Vera, and Bellamy, Rachel K. E.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em> Jan 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model’s to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people’s trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI’s errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AIES</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_joint_2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Joint Optimization of AI Fairness and Utility: A Human-Centered Approach</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, Bellamy, Rachel, and Varshney, Kush
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em> Feb 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers’ preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="arya_one_2019" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques</div>
          <!-- Author -->
          <div class="author">Arya, Vijay, Bellamy, Rachel K. E., Chen, Pin-Yu, Dhurandhar, Amit, Hind, Michael, Hoffman, Samuel C., Houde, Stephanie, Liao, Q. Vera, Luss, Ronny, Mojsilović, Aleksandra, Mourad, Sami, Pedemonte, Pablo, Raghavendra, Ramya, Richards, John, Sattigeri, Prasanna, Shanmugam, Karthikeyan, Singh, Moninder, Varshney, Kush R., Wei, Dennis, and <em>Zhang, Yunfeng</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv:1909.03012 [cs, stat]</em> Sep 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>As artificial intelligence and machine learning algorithms make further inroads into society, calls are increasing from multiple stakeholders for these algorithms to explain their outputs. At the same time, these stakeholders, whether they be affected citizens, government regulators, domain experts, or system developers, present different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360 (http://aix360.mybluemix.net/), an open-source software toolkit featuring eight diverse and state-of-the-art explainability methods and two evaluation metrics. Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. We also discuss enhancements to bring research innovations closer to consumers of explanations, ranging from simplified, more accessible versions of algorithms, to tutorials and an interactive web demo to introduce AI explainability to different audiences and application domains. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IUI</abbr></div>

        <!-- Entry bib key -->
        <div id="dodge_explaining_2019" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment</div>
          <!-- Author -->
          <div class="author">Dodge, Jonathan, Liao, Q. Vera, 
                  <em>Zhang, Yunfeng</em>, Bellamy, Rachel K. E., and Dugan, Casey
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 24th International Conference on Intelligent User Interfaces  - IUI ’19</em> Sep 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="bellamy_ai_2018" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias</div>
          <!-- Author -->
          <div class="author">Bellamy, Rachel K. E., Dey, Kuntal, Hind, Michael, Hoffman, Samuel C., Houde, Stephanie, Kannan, Kalapriya, Lohia, Pranay, Martino, Jacquelyn, Mehta, Sameep, Mojsilovic, Aleksandra, Nagar, Seema, Ramamurthy, Karthikeyan Natesan, Richards, John, Saha, Diptikalyan, Sattigeri, Prasanna, Singh, Moninder, Varshney, Kush R., and <em>Zhang, Yunfeng</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv:1810.01943 [cs]</em> Oct 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (https://aif360.mybluemix.net) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.\vphantom}</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="shrinivasan_celio:_2017" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">CELIO: An Application Development Framework for Interactive Spaces</div>
          <!-- Author -->
          <div class="author">Shrinivasan, Yedendra B., and <em>Zhang, Yunfeng</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:1710.01772</em> Oct 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_combining_2017" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Combining Absolute and Relative Pointing for Fast and Accurate Distant Interaction</div>
          <!-- Author -->
          <div class="author">
                <em>Zhang, Yunfeng</em>
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:1710.01778</em> Oct 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="shrinivasan_celio:_2018" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">CELIO: An Application Development Framework for Interactive Spaces</div>
          <!-- Author -->
          <div class="author">Shrinivasan, Yedendra B., and <em>Zhang, Yunfeng</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:1710.01772</em> Oct 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_combining_2018" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Combining Absolute and Relative Pointing for Fast and Accurate Distant Interaction</div>
          <!-- Author -->
          <div class="author">
                <em>Zhang, Yunfeng</em>
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:1710.01778</em> Oct 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICCM</abbr></div>

        <!-- Entry bib key -->
        <div id="kieras_visual_2015" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Visual Search of Displays of Many Objects: Modeling Detailed Eye Movement Effects with Improved Epic</div>
          <!-- Author -->
          <div class="author">Kieras, David E., Hornof, A. J., and Zhang, Yongfeng
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 13th International Conference on Cognitive Modeling</em> Oct 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="shrinivasan_interacting_2015" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Interacting with Large Display Walls in Commercial Environments</div>
          <!-- Author -->
          <div class="author">Shrinivasan, Y., Melville, D., 
                  <em>Zhang, Yunfeng</em>, Lenchner, Jonathan, and Bellamy, R.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Commercial Environments, Workshop on Interaction on Large Displays, ACM ITS 2015</em> Oct 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CHI</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_designing_2015" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Designing Information for Remediating Cognitive Biases in Decision-Making</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, Bellamy, Rachel KE, and Kellogg, Wendy A.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em> Oct 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TOPICS</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_reinforcement_2015" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Reinforcement Learning and Counterfactual Reasoning Explain Adaptive Behavior in a Changing Environment</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, Paik, Jaehyon, and Pirolli, Peter
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Topics in cognitive science</em> Oct 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="kieras_visual_2016" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Visual Search of Displays of Many Objects: Modeling Detailed Eye Movement Effects with Improved Epic</div>
          <!-- Author -->
          <div class="author">Kieras, David E., Hornof, A. J., and Zhang, Yongfeng
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 13th International Conference on Cognitive Modeling</em> Oct 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="shrinivasan_interacting_2016" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Interacting with Large Display Walls in Commercial Environments</div>
          <!-- Author -->
          <div class="author">Shrinivasan, Y., Melville, D., 
                  <em>Zhang, Yunfeng</em>, Lenchner, Jonathan, and Bellamy, R.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Commercial Environments, Workshop on Interaction on Large Displays, ACM ITS 2015</em> Oct 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CHI</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_designing_2016" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Designing Information for Remediating Cognitive Biases in Decision-Making</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, Bellamy, Rachel KE, and Kellogg, Wendy A.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em> Oct 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TOPICS</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_reinforcement_2016" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Reinforcement Learning and Counterfactual Reasoning Explain Adaptive Behavior in a Changing Environment</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, Paik, Jaehyon, and Pirolli, Peter
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Topics in cognitive science</em> Oct 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2014</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">BICA</abbr></div>

        <!-- Entry bib key -->
        <div id="paik_counterfactual_2014" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Counterfactual Reasoning as a Key for Explaining Adaptive Behavior in a Changing Environment</div>
          <!-- Author -->
          <div class="author">Paik, Jaehyon, 
                  <em>Zhang, Yunfeng</em>, and Pirolli, Peter
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Biologically Inspired Cognitive Architectures</em> Oct 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ETRA</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_easy_2014" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Easy Post-Hoc Spatial Recalibration of Eye Tracking Data</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, and Hornof, Anthony J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the Symposium on Eye Tracking Research and Applications</em> Oct 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CHI</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_understanding_2014" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Understanding Multitasking through Parallelized Strategy Exploration and Individualized Cognitive Modeling</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, and Hornof, Anthony J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> Oct 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">BICA</abbr></div>

        <!-- Entry bib key -->
        <div id="paik_counterfactual_2015" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Counterfactual Reasoning as a Key for Explaining Adaptive Behavior in a Changing Environment</div>
          <!-- Author -->
          <div class="author">Paik, Jaehyon, 
                  <em>Zhang, Yunfeng</em>, and Pirolli, Peter
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Biologically Inspired Cognitive Architectures</em> Oct 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ETRA</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_easy_2015" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Easy Post-Hoc Spatial Recalibration of Eye Tracking Data</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, and Hornof, Anthony J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the Symposium on Eye Tracking Research and Applications</em> Oct 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CHI</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_understanding_2015" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Understanding Multitasking through Parallelized Strategy Exploration and Individualized Cognitive Modeling</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, and Hornof, Anthony J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> Oct 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2013</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICCM</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_using_2013" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Using Model Tracing and Evolutionary Algorithms to Determine Parameter Settings for Cognitive Models from Time Series Data Such as Visual Scanpaths</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, and Hornof, Anthony J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 12th International Conference on Cognitive Modeling</em> Oct 2013
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICCM</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_using_2014" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Using Model Tracing and Evolutionary Algorithms to Determine Parameter Settings for Cognitive Models from Time Series Data Such as Visual Scanpaths</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, and Hornof, Anthony J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 12th International Conference on Cognitive Modeling</em> Oct 2013
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2012</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">HFES</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_discrete_2012" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Discrete Movement Model For Cursor Tracking Validated in the Context of a Dual-Task Experiment</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, and Hornof, Anthony J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em> Oct 2012
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">HFES</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_discrete_2013" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Discrete Movement Model For Cursor Tracking Validated in the Context of a Dual-Task Experiment</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, and Hornof, Anthony J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em> Oct 2012
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2011</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">BRM</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_mode--disparities_2011" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Mode-of-Disparities Error Correction of Eye-Tracking Data</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, and Hornof, Anthony J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Behavior research methods</em> Oct 2011
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">BRM</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang_mode--disparities_2012" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Mode-of-Disparities Error Correction of Eye-Tracking Data</div>
          <!-- Author -->
          <div class="author">
                  <em>Zhang, Yunfeng</em>, and Hornof, Anthony J.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Behavior research methods</em> Oct 2011
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2010</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">CHI</abbr></div>

        <!-- Entry bib key -->
        <div id="hornof_knowing_2010" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Knowing Where and When to Look in a Time-Critical Multimodal Dual Task</div>
          <!-- Author -->
          <div class="author">Hornof, Anthony J., 
                  <em>Zhang, Yunfeng</em>, and Halverson, Tim
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> Oct 2010
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICCM</abbr></div>

        <!-- Entry bib key -->
        <div id="hornof_task-constrained_2010" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Task-Constrained Interleaving of Perceptual and Motor Processes in a Time-Critical Dual Task as Revealed through Eye Tracking</div>
          <!-- Author -->
          <div class="author">Hornof, Anthony J., and <em>Zhang, Yunfeng</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 10th International Conference on Cognitive Modeling</em> Oct 2010
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
          </div>

          
        </div>
      </div>
</li>
</ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Yunfeng  Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

