---
---

@article{arya_one_2019,
  title = {One {{Explanation Does Not Fit All}}: {{A Toolkit}} and {{Taxonomy}} of {{AI Explainability Techniques}}},
  shorttitle = {One {{Explanation Does Not Fit All}}},
  author = {Arya, Vijay and Bellamy, Rachel K. E. and Chen, Pin-Yu and Dhurandhar, Amit and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Liao, Q. Vera and Luss, Ronny and Mojsilovi{\'c}, Aleksandra and Mourad, Sami and Pedemonte, Pablo and Raghavendra, Ramya and Richards, John and Sattigeri, Prasanna and Shanmugam, Karthikeyan and Singh, Moninder and Varshney, Kush R. and Wei, Dennis and Zhang, Yunfeng},
  year = {2019},
  month = sep,
  url = {http://arxiv.org/abs/1909.03012},
  urldate = {2021-02-03},
  abbr = {arXiv},
  abstract = {As artificial intelligence and machine learning algorithms make further inroads into society, calls are increasing from multiple stakeholders for these algorithms to explain their outputs. At the same time, these stakeholders, whether they be affected citizens, government regulators, domain experts, or system developers, present different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360 (http://aix360.mybluemix.net/), an open-source software toolkit featuring eight diverse and state-of-the-art explainability methods and two evaluation metrics. Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. We also discuss enhancements to bring research innovations closer to consumers of explanations, ranging from simplified, more accessible versions of algorithms, to tutorials and an interactive web demo to introduce AI explainability to different audiences and application domains. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1909.03012},
  eprinttype = {arxiv},
  file = {/Users/yz/Zotero/storage/2WIGCVGP/Arya et al. - 2019 - One Explanation Does Not Fit All A Toolkit and Ta.pdf},
  journal = {arXiv:1909.03012 [cs, stat]},
  primaryclass = {cs, stat}
}

@article{bellamy_ai_2018,
  title = {{{AI Fairness}} 360: {{An Extensible Toolkit}} for {{Detecting}}, {{Understanding}}, and {{Mitigating Unwanted Algorithmic Bias}}},
  shorttitle = {{{AI Fairness}} 360},
  author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
  year = {2018},
  month = oct,
  url = {http://arxiv.org/abs/1810.01943},
  urldate = {2020-05-23},
  abbr = {arXiv},
  abstract = {Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license \{https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (https://aif360.mybluemix.net) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.\vphantom\}},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1810.01943},
  eprinttype = {arxiv},
  file = {/Users/yz/Dropbox/zotero files/Bellamy et al_2018_AI Fairness 360.pdf},
  journal = {arXiv:1810.01943 [cs]},
  primaryclass = {cs},
  selected = {true}
}

@inproceedings{dodge_explaining_2019,
  title = {Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment},
  shorttitle = {Explaining Models},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}  - {{IUI}} '19},
  author = {Dodge, Jonathan and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel K. E. and Dugan, Casey},
  year = {2019},
  pages = {275--285},
  publisher = {{ACM Press}},
  address = {{Marina del Ray, California}},
  doi = {10/ggcpsp},
  abbr = {IUI},
  award = {Outstanding Paper},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Dodge et al_2019_Explaining models.pdf;/Users/yz/Dropbox/zotero files/Dodge et al_2019_Explaining models2.pdf},
  isbn = {978-1-4503-6272-6},
  language = {en},
  selected = {true}
}

@article{ghai_explainable_2021,
  title = {Explainable {{Active Learning}} ({{XAL}}): {{Toward AI Explanations}} as {{Interfaces}} for {{Machine Teachers}}},
  shorttitle = {Explainable {{Active Learning}} ({{XAL}})},
  author = {Ghai, Bhavya and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel and Mueller, Klaus},
  year = {2021},
  month = jan,
  volume = {4},
  pages = {235:1--235:28},
  doi = {10/gh7kqj},
  abbr = {CSCW},
  abstract = {The wide adoption of Machine Learning (ML) technologies has created a growing demand for people who can train ML models. Some advocated the term "machine teacher'' to refer to the role of people who inject domain knowledge into ML models. This "teaching'' perspective emphasizes supporting the productivity and mental wellbeing of machine teachers through efficient learning algorithms and thoughtful design of human-AI interfaces. One promising learning paradigm is Active Learning (AL), by which the model intelligently selects instances to query a machine teacher for labels, so that the labeling workload could be largely reduced. However, in current AL settings, the human-AI interface remains minimal and opaque. A dearth of empirical studies further hinders us from developing teacher-friendly interfaces for AL algorithms. In this work, we begin considering AI explanations as a core element of the human-AI interface for teaching machines. When a human student learns, it is a common pattern to present one's own reasoning and solicit feedback from the teacher. When a ML model learns and still makes mistakes, the teacher ought to be able to understand the reasoning underlying its mistakes. When the model matures, the teacher should be able to recognize its progress in order to trust and feel confident about their teaching outcome. Toward this vision, we propose a novel paradigm of explainable active learning (XAL), by introducing techniques from the surging field of explainable AI (XAI) into an AL setting. We conducted an empirical study comparing the model learning outcomes, feedback content and experience with XAL, to that of traditional AL and coactive learning (providing the model's prediction without explanation). Our study shows benefits of AI explanation as interfaces for machine teaching--supporting trust calibration and enabling rich forms of teaching feedback, and potential drawbacks--anchoring effect with the model judgment and additional cognitive workload. Our study also reveals important individual factors that mediate a machine teacher's reception to AI explanations, including task knowledge, AI experience and Need for Cognition. By reflecting on the results, we suggest future directions and design implications for XAL, and more broadly, machine teaching through AI explanations.},
  copyright = {All rights reserved},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  number = {CSCW3}
}

@inproceedings{hornof_knowing_2010,
  title = {Knowing Where and When to Look in a Time-Critical Multimodal Dual Task},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on Human Factors in Computing Systems},
  author = {Hornof, Anthony J. and Zhang, Yunfeng and Halverson, Tim},
  year = {2010},
  pages = {2103--2112},
  publisher = {{ACM}},
  doi = {10.1145/1753326.1753647},
  abbr = {CHI},
  award = {Honourable Mention},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Hornof et al_2010_Knowing where and when to look in a time-critical multimodal dual task.pdf}
}

@inproceedings{hornof_task-constrained_2010,
  title = {Task-Constrained Interleaving of Perceptual and Motor Processes in a Time-Critical Dual Task as Revealed through Eye Tracking},
  booktitle = {Proceedings of the 10th International Conference on Cognitive Modeling},
  author = {Hornof, Anthony J. and Zhang, Yunfeng},
  year = {2010},
  pages = {97--102},
  publisher = {{Citeseer}},
  abbr = {ICCM},
  award = {Best Applied Paper},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Hornof_Zhang_2010_Task-constrained interleaving of perceptual and motor processes in a.pdf}
}

@inproceedings{kieras_visual_2015,
  title = {Visual Search of Displays of Many Objects: {{Modeling}} Detailed Eye Movement Effects with Improved Epic},
  shorttitle = {Visual Search of Displays of Many Objects},
  booktitle = {Proceedings of the 13th International Conference on Cognitive Modeling},
  author = {Kieras, David E. and Hornof, A. J. and Zhang, Yongfeng},
  year = {2015},
  volume = {55},
  abbr = {ICCM},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Kieras et al_2015_Visual search of displays of many objects.pdf}
}

@inproceedings{narkar_model_2021,
  title = {Model {{LineUpper}}: {{Supporting Interactive Model Comparison}} at {{Multiple Levels}} for {{AutoML}}},
  shorttitle = {Model {{LineUpper}}},
  booktitle = {26th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Narkar, Shweta and Zhang, Yunfeng and Liao, Q. Vera and Wang, Dakuo and Weisz, Justin D.},
  year = {2021},
  month = apr,
  pages = {170--174},
  publisher = {{ACM}},
  address = {{College Station TX USA}},
  doi = {10/gjqfvc},
  abbr = {IUI},
  copyright = {All rights reserved},
  isbn = {978-1-4503-8017-1},
  language = {en}
}

@inproceedings{natesan_ramamurthy_model_2020,
  title = {Model Agnostic Multilevel Explanations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Natesan Ramamurthy, Karthikeyan and Vinzamuri, Bhanukiran and Zhang, Yunfeng and Dhurandhar, Amit},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {5968--5979},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/file/426f990b332ef8193a61cc90516c1245-Paper.pdf},
  abbr = {NeurIPS},
  copyright = {All rights reserved},
  keywords = {⛔ No DOI found},
  selected = {true}
}

@article{paik_counterfactual_2014,
  title = {Counterfactual Reasoning as a Key for Explaining Adaptive Behavior in a Changing Environment},
  author = {Paik, Jaehyon and Zhang, Yunfeng and Pirolli, Peter},
  year = {2014},
  volume = {10},
  pages = {24--29},
  doi = {10.1016/j.bica.2014.11.004},
  abbr = {BICA},
  copyright = {All rights reserved},
  journal = {Biologically Inspired Cognitive Architectures}
}

@inproceedings{sharma_data_2020,
  title = {Data {{Augmentation}} for {{Discrimination Prevention}} and {{Bias Disambiguation}}},
  booktitle = {Proceedings of the {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Sharma, Shubham and Zhang, Yunfeng and R{\'i}os Aliaga, Jes{\'u}s M. and Bouneffouf, Djallel and Muthusamy, Vinod and Varshney, Kush R.},
  year = {2020},
  month = feb,
  pages = {358--364},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10/ghj6jt},
  abbr = {AIES},
  abstract = {Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an "ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.},
  copyright = {All rights reserved},
  file = {/Users/yz/Zotero/storage/HRW6QKSR/Sharma et al. - 2020 - Data Augmentation for Discrimination Prevention an.pdf},
  isbn = {978-1-4503-7110-0},
  series = {{{AIES}} '20}
}

@article{shrinivasan_celio:_2017,
  ids = {shrinivasan_celio_2017-1},
  title = {{{CELIO}}: {{An}} Application Development Framework for Interactive Spaces},
  shorttitle = {{{CELIO}}},
  author = {Shrinivasan, Yedendra B. and Zhang, Yunfeng},
  year = {2017},
  abbr = {arXiv},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1710.01772},
  eprinttype = {arxiv},
  file = {/Users/yz/Dropbox/zotero files/Shrinivasan_Zhang_2017_CELIO.pdf;/Users/yz/Dropbox/zotero files/Shrinivasan_Zhang_2017_CELIO2.pdf},
  journal = {arXiv preprint arXiv:1710.01772}
}

@inproceedings{shrinivasan_interacting_2015,
  title = {Interacting with {{Large Display Walls}} in {{Commercial Environments}}},
  booktitle = {Commercial {{Environments}}, {{Workshop}} on {{Interaction}} on {{Large Displays}}, {{ACM ITS}} 2015},
  author = {Shrinivasan, Y. and Melville, D. and Zhang, Yunfeng and Lenchner, Jonathan and Bellamy, R.},
  year = {2015},
  abbr = {arXiv},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Shrinivasan et al_2015_Interacting with Large Display Walls in Commercial Environments.pdf}
}

@article{zhang_combining_2017,
  ids = {zhang_combining_2017-1},
  title = {Combining Absolute and Relative Pointing for Fast and Accurate Distant Interaction},
  author = {Zhang, Yunfeng},
  year = {2017},
  abbr = {arXiv},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1710.01778},
  eprinttype = {arxiv},
  file = {/Users/yz/Dropbox/zotero files/Zhang_2017_Combining absolute and relative pointing for fast and accurate distant.pdf;/Users/yz/Dropbox/zotero files/Zhang_2017_Combining absolute and relative pointing for fast and accurate distant2.pdf},
  journal = {arXiv preprint arXiv:1710.01778}
}

@inproceedings{zhang_designing_2015,
  title = {Designing Information for Remediating Cognitive Biases in Decision-Making},
  booktitle = {Proceedings of the 33rd Annual {{ACM}} Conference on Human Factors in Computing Systems},
  author = {Zhang, Yunfeng and Bellamy, Rachel KE and Kellogg, Wendy A.},
  year = {2015},
  pages = {2211--2220},
  publisher = {{ACM}},
  doi = {10.1145/2702123.2702239},
  abbr = {CHI},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang et al_2015_Designing information for remediating cognitive biases in decision-making.pdf}
}

@inproceedings{zhang_discrete_2012,
  title = {A {{Discrete Movement Model For Cursor Tracking Validated}} in the {{Context}} of a {{Dual}}-{{Task Experiment}}},
  booktitle = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  author = {Zhang, Yunfeng and Hornof, Anthony J.},
  year = {2012},
  volume = {56},
  pages = {1000--1004},
  publisher = {{SAGE Publications Sage CA: Los Angeles, CA}},
  abbr = {HFES},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang_Hornof_2012_A Discrete Movement Model For Cursor Tracking Validated in the Context of a.pdf}
}

@inproceedings{zhang_easy_2014,
  title = {Easy Post-Hoc Spatial Recalibration of Eye Tracking Data},
  booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  author = {Zhang, Yunfeng and Hornof, Anthony J.},
  year = {2014},
  pages = {95--98},
  publisher = {{ACM}},
  doi = {10.1145/2578153.2578166},
  abbr = {ETRA},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang_Hornof_2014_Easy post-hoc spatial recalibration of eye tracking data.pdf}
}

@inproceedings{zhang_effect_2020,
  title = {Effect of Confidence and Explanation on Accuracy and Trust Calibration in {{AI}}-Assisted Decision Making},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Zhang, Yunfeng and Liao, Q. Vera and Bellamy, Rachel K. E.},
  year = {2020},
  month = jan,
  pages = {295--305},
  publisher = {{Association for Computing Machinery}},
  address = {{Barcelona, Spain}},
  doi = {10/ggjpcr},
  abbr = {FAccT},
  abstract = {Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang et al_2020_Effect of confidence and explanation on accuracy and trust calibration in.pdf},
  isbn = {978-1-4503-6936-7},
  selected = {true},
  series = {{{FAT}}* '20}
}

@inproceedings{zhang_joint_2020,
  title = {Joint {{Optimization}} of {{AI Fairness}} and {{Utility}}: {{A Human}}-{{Centered Approach}}},
  shorttitle = {Joint {{Optimization}} of {{AI Fairness}} and {{Utility}}},
  booktitle = {Proceedings of the {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Zhang, Yunfeng and Bellamy, Rachel and Varshney, Kush},
  year = {2020},
  month = feb,
  pages = {400--406},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10/ggww4d},
  abbr = {AIES},
  abstract = {Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang et al_2020_Joint Optimization of AI Fairness and Utility.pdf},
  isbn = {978-1-4503-7110-0},
  selected = {true},
  series = {{{AIES}} '20}
}

@article{zhang_mode--disparities_2011,
  title = {Mode-of-Disparities Error Correction of Eye-Tracking Data},
  author = {Zhang, Yunfeng and Hornof, Anthony J.},
  year = {2011},
  volume = {43},
  pages = {834--842},
  doi = {10.3758/s13428-011-0073-0},
  abbr = {BRM},
  copyright = {All rights reserved},
  journal = {Behavior research methods},
  number = {3}
}

@article{zhang_reinforcement_2015,
  title = {Reinforcement Learning and Counterfactual Reasoning Explain Adaptive Behavior in a Changing Environment},
  author = {Zhang, Yunfeng and Paik, Jaehyon and Pirolli, Peter},
  year = {2015},
  volume = {7},
  pages = {368--381},
  doi = {10.1111/tops.12143},
  abbr = {TOPICS},
  copyright = {All rights reserved},
  journal = {Topics in cognitive science},
  number = {2}
}

@inproceedings{zhang_understanding_2014,
  title = {Understanding Multitasking through Parallelized Strategy Exploration and Individualized Cognitive Modeling},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on Human Factors in Computing Systems},
  author = {Zhang, Yunfeng and Hornof, Anthony J.},
  year = {2014},
  pages = {3885--3894},
  publisher = {{ACM}},
  doi = {10.1145/2556288.2557351},
  abbr = {CHI},
  award = {Best Paper},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang_Hornof_2014_Understanding multitasking through parallelized strategy exploration and.pdf},
  selected = {true}
}

@inproceedings{zhang_using_2013,
  title = {Using Model Tracing and Evolutionary Algorithms to Determine Parameter Settings for Cognitive Models from Time Series Data Such as Visual Scanpaths},
  booktitle = {Proceedings of the 12th International Conference on Cognitive Modeling},
  author = {Zhang, Yunfeng and Hornof, Anthony J.},
  year = {2013},
  pages = {433--438},
  abbr = {ICCM},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang_Hornof_2013_Using model tracing and evolutionary algorithms to determine parameter settings.pdf}
}

@inproceedings{kieras_visual_2015,
  title = {Visual Search of Displays of Many Objects: {{Modeling}} Detailed Eye Movement Effects with Improved Epic},
  shorttitle = {Visual Search of Displays of Many Objects},
  booktitle = {Proceedings of the 13th International Conference on Cognitive Modeling},
  author = {Kieras, David E. and Hornof, A. J. and Zhang, Yongfeng},
  year = {2015},
  volume = {55},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Kieras et al_2015_Visual search of displays of many objects.pdf}
}

@inproceedings{narkar_model_2021,
  title = {Model {{LineUpper}}: {{Supporting Interactive Model Comparison}} at {{Multiple Levels}} for {{AutoML}}},
  shorttitle = {Model {{LineUpper}}},
  booktitle = {26th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Narkar, Shweta and Zhang, Yunfeng and Liao, Q. Vera and Wang, Dakuo and Weisz, Justin D.},
  year = {2021},
  month = apr,
  pages = {170--174},
  publisher = {{ACM}},
  address = {{College Station TX USA}},
  doi = {10/gjqfvc},
  abbr = {IUI},
  copyright = {All rights reserved},
  isbn = {978-1-4503-8017-1},
  language = {en}
}

@inproceedings{natesan_ramamurthy_model_2020,
  title = {Model Agnostic Multilevel Explanations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Natesan Ramamurthy, Karthikeyan and Vinzamuri, Bhanukiran and Zhang, Yunfeng and Dhurandhar, Amit},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {5968--5979},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/file/426f990b332ef8193a61cc90516c1245-Paper.pdf},
  abbr = {NeurIPS},
  copyright = {All rights reserved},
  keywords = {⛔ No DOI found}
}

@article{paik_counterfactual_2014,
  title = {Counterfactual Reasoning as a Key for Explaining Adaptive Behavior in a Changing Environment},
  author = {Paik, Jaehyon and Zhang, Yunfeng and Pirolli, Peter},
  year = {2014},
  volume = {10},
  pages = {24--29},
  doi = {10.1016/j.bica.2014.11.004},
  abbr = {BICA},
  copyright = {All rights reserved},
  journal = {Biologically Inspired Cognitive Architectures}
}

@inproceedings{sharma_data_2020,
  title = {Data {{Augmentation}} for {{Discrimination Prevention}} and {{Bias Disambiguation}}},
  booktitle = {Proceedings of the {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Sharma, Shubham and Zhang, Yunfeng and R{\'i}os Aliaga, Jes{\'u}s M. and Bouneffouf, Djallel and Muthusamy, Vinod and Varshney, Kush R.},
  year = {2020},
  month = feb,
  pages = {358--364},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10/ghj6jt},
  abbr = {AIES},
  abstract = {Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an "ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.},
  copyright = {All rights reserved},
  file = {/Users/yz/Zotero/storage/HRW6QKSR/Sharma et al. - 2020 - Data Augmentation for Discrimination Prevention an.pdf},
  isbn = {978-1-4503-7110-0},
  series = {{{AIES}} '20}
}

@article{shrinivasan_celio:_2017,
  ids = {shrinivasan_celio_2017-1},
  title = {{{CELIO}}: {{An}} Application Development Framework for Interactive Spaces},
  shorttitle = {{{CELIO}}},
  author = {Shrinivasan, Yedendra B. and Zhang, Yunfeng},
  year = {2017},
  abbr = {arXiv},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1710.01772},
  eprinttype = {arxiv},
  file = {/Users/yz/Dropbox/zotero files/Shrinivasan_Zhang_2017_CELIO.pdf;/Users/yz/Dropbox/zotero files/Shrinivasan_Zhang_2017_CELIO2.pdf},
  journal = {arXiv preprint arXiv:1710.01772}
}

@inproceedings{shrinivasan_interacting_2015,
  title = {Interacting with {{Large Display Walls}} in {{Commercial Environments}}},
  booktitle = {Commercial {{Environments}}, {{Workshop}} on {{Interaction}} on {{Large Displays}}, {{ACM ITS}} 2015},
  author = {Shrinivasan, Y. and Melville, D. and Zhang, Yunfeng and Lenchner, Jonathan and Bellamy, R.},
  year = {2015},
  abbr = {arXiv},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Shrinivasan et al_2015_Interacting with Large Display Walls in Commercial Environments.pdf}
}

@article{zhang_combining_2017,
  ids = {zhang_combining_2017-1},
  title = {Combining Absolute and Relative Pointing for Fast and Accurate Distant Interaction},
  author = {Zhang, Yunfeng},
  year = {2017},
  abbr = {arXiv},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1710.01778},
  eprinttype = {arxiv},
  file = {/Users/yz/Dropbox/zotero files/Zhang_2017_Combining absolute and relative pointing for fast and accurate distant.pdf;/Users/yz/Dropbox/zotero files/Zhang_2017_Combining absolute and relative pointing for fast and accurate distant2.pdf},
  journal = {arXiv preprint arXiv:1710.01778}
}

@inproceedings{zhang_designing_2015,
  title = {Designing Information for Remediating Cognitive Biases in Decision-Making},
  booktitle = {Proceedings of the 33rd Annual {{ACM}} Conference on Human Factors in Computing Systems},
  author = {Zhang, Yunfeng and Bellamy, Rachel KE and Kellogg, Wendy A.},
  year = {2015},
  pages = {2211--2220},
  publisher = {{ACM}},
  doi = {10.1145/2702123.2702239},
  abbr = {CHI},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang et al_2015_Designing information for remediating cognitive biases in decision-making.pdf}
}

@inproceedings{zhang_discrete_2012,
  title = {A {{Discrete Movement Model For Cursor Tracking Validated}} in the {{Context}} of a {{Dual}}-{{Task Experiment}}},
  booktitle = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  author = {Zhang, Yunfeng and Hornof, Anthony J.},
  year = {2012},
  volume = {56},
  pages = {1000--1004},
  publisher = {{SAGE Publications Sage CA: Los Angeles, CA}},
  abbr = {HFES},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang_Hornof_2012_A Discrete Movement Model For Cursor Tracking Validated in the Context of a.pdf}
}

@inproceedings{zhang_easy_2014,
  title = {Easy Post-Hoc Spatial Recalibration of Eye Tracking Data},
  booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  author = {Zhang, Yunfeng and Hornof, Anthony J.},
  year = {2014},
  pages = {95--98},
  publisher = {{ACM}},
  doi = {10.1145/2578153.2578166},
  abbr = {ETRA},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang_Hornof_2014_Easy post-hoc spatial recalibration of eye tracking data.pdf}
}

@inproceedings{zhang_effect_2020,
  title = {Effect of Confidence and Explanation on Accuracy and Trust Calibration in {{AI}}-Assisted Decision Making},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Zhang, Yunfeng and Liao, Q. Vera and Bellamy, Rachel K. E.},
  year = {2020},
  month = jan,
  pages = {295--305},
  publisher = {{Association for Computing Machinery}},
  address = {{Barcelona, Spain}},
  doi = {10/ggjpcr},
  abbr = {FAccT},
  abstract = {Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang et al_2020_Effect of confidence and explanation on accuracy and trust calibration in.pdf},
  isbn = {978-1-4503-6936-7},
  selected = {true},
  series = {{{FAT}}* '20}
}

@inproceedings{zhang_joint_2020,
  title = {Joint {{Optimization}} of {{AI Fairness}} and {{Utility}}: {{A Human}}-{{Centered Approach}}},
  shorttitle = {Joint {{Optimization}} of {{AI Fairness}} and {{Utility}}},
  booktitle = {Proceedings of the {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Zhang, Yunfeng and Bellamy, Rachel and Varshney, Kush},
  year = {2020},
  month = feb,
  pages = {400--406},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10/ggww4d},
  abbr = {AIES},
  abstract = {Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang et al_2020_Joint Optimization of AI Fairness and Utility.pdf},
  isbn = {978-1-4503-7110-0},
  selected = {true},
  series = {{{AIES}} '20}
}

@article{zhang_mode--disparities_2011,
  title = {Mode-of-Disparities Error Correction of Eye-Tracking Data},
  author = {Zhang, Yunfeng and Hornof, Anthony J.},
  year = {2011},
  volume = {43},
  pages = {834--842},
  doi = {10.3758/s13428-011-0073-0},
  abbr = {BRM},
  copyright = {All rights reserved},
  journal = {Behavior research methods},
  number = {3}
}

@article{zhang_reinforcement_2015,
  title = {Reinforcement Learning and Counterfactual Reasoning Explain Adaptive Behavior in a Changing Environment},
  author = {Zhang, Yunfeng and Paik, Jaehyon and Pirolli, Peter},
  year = {2015},
  volume = {7},
  pages = {368--381},
  doi = {10.1111/tops.12143},
  abbr = {TOPICS},
  copyright = {All rights reserved},
  journal = {Topics in cognitive science},
  number = {2}
}

@inproceedings{zhang_understanding_2014,
  title = {Understanding Multitasking through Parallelized Strategy Exploration and Individualized Cognitive Modeling},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on Human Factors in Computing Systems},
  author = {Zhang, Yunfeng and Hornof, Anthony J.},
  year = {2014},
  pages = {3885--3894},
  publisher = {{ACM}},
  doi = {10.1145/2556288.2557351},
  abbr = {CHI},
  award = {Best Paper},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang_Hornof_2014_Understanding multitasking through parallelized strategy exploration and.pdf},
  selected = {true}
}

@inproceedings{zhang_using_2013,
  title = {Using Model Tracing and Evolutionary Algorithms to Determine Parameter Settings for Cognitive Models from Time Series Data Such as Visual Scanpaths},
  booktitle = {Proceedings of the 12th International Conference on Cognitive Modeling},
  author = {Zhang, Yunfeng and Hornof, Anthony J.},
  year = {2013},
  pages = {433--438},
  abbr = {ICCM},
  copyright = {All rights reserved},
  file = {/Users/yz/Dropbox/zotero files/Zhang_Hornof_2013_Using model tracing and evolutionary algorithms to determine parameter settings.pdf}
}
